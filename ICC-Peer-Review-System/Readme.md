# AI System D002_Peer Review

## System Information

**Document ID:** AI System D002_Peer Review  

## Overview

AI System "D002_Peer_Review" is an AI-powered grant proposal review system that generates structured peer reviews for research proposals before submission to federal funding agencies. The system implements two distinct architectural approaches for document analysis and review generation.

## Architecture

### Core Components

**LLM Backend**: ChatOllama with `gemma3` model  
**Embedding Model**: Ollama `nomic-embed-text` (768-dimensional embeddings)  
**Tokenizer**: Google Gemma-3-4b-it tokenizer for token counting  
**Vector Store**: FAISS for efficient similarity search  
**Document Loader**: LangChain PyPDFLoader  
**UI Framework**: Gradio with file upload and dropdown components

### Two Processing Pipelines

#### 1. No-RAG Approach (Direct Context)

This approach processes the entire document as a single context window:

```
PDF → PyPDFLoader → Full Text Extraction → Token Limit Check → 
ChatPromptTemplate → ChatOllama (gemma3) → Structured Review
```

**Token Management:**
- Maximum context: 128,000 tokens
- Tokenization: Google Gemma-3-4b-it tokenizer
- Truncation: Automatic truncation at token limit with warning flag
- Token counting: Pre-processing validation using `AutoTokenizer.encode()`

**Advantages:**
- Complete document context preserved
- No information loss from chunking
- Simpler pipeline with fewer components

**Limitations:**
- Constrained by model context window
- Higher memory consumption
- Potential truncation for large documents

#### 2. RAG Approach (Retrieval-Augmented Generation)

This approach uses document chunking and semantic retrieval:

```
PDF → PyPDFLoader → Text Splitting → Document Chunking →
Embedding Generation → FAISS Vector Store → MultiQueryRetriever →
Context Assembly → RAG Chain → Structured Review
```

**Text Chunking Strategy:**
- Splitter: `RecursiveCharacterTextSplitter`
- Chunk size: 500 characters
- Chunk overlap: 50 characters
- Document structure: Paragraph-based splitting (`\n\n` delimiter)

**Retrieval Configuration:**
- Retriever: `MultiQueryRetriever` with query expansion
- Search strategy: Retrieve all chunks (`k=len(chunks)`)
- Embedding dimension: 768 (nomic-embed-text)
- Vector store: FAISS with L2 distance metric

**Query Expansion:**
- Custom `PromptTemplate` for query generation
- Multiple query variants generated by LLM
- Comprehensive context retrieval across all chunks

**Advantages:**
- Handles arbitrarily large documents
- Semantic search for relevant sections
- Scalable to large corpora

**Limitations:**
- Potential context fragmentation
- Dependency on embedding quality
- Additional processing overhead

## Technical Implementation

### Document Processing Pipeline

**PDF Text Extraction:**
```python
PyPDFLoader → List[Document] → Text concatenation with "\n\n" separators
```
Returns: Full text string and page count

**Token Validation:**
- Encodes text using Gemma tokenizer
- Compares against 128K token threshold
- Returns: (processed_text, truncation_flag, token_count)

### Prompt Engineering

Both approaches use structured prompts with four mandatory sections:

1. **SUMMARY**: 4-5 sentence overview of objectives, approach, and significance
2. **KEY POINTS**: Bullet-point strengths (methodology, innovation, feasibility, impact)
3. **RECOMMENDATIONS**: Bullet-point weaknesses and actionable improvements
4. **OVERALL EVALUATION**: Quality assessment, funding recommendation, justification

**Funding Recommendation Scale:**
- Strongly Recommend
- Recommend
- Conditional
- Not Recommend

### RAG Chain Construction

**LangChain LCEL Syntax:**
```python
{
    "context": retriever,
    "question": RunnablePassthrough()
} | prompt | llm | StrOutputParser()
```

**Execution Flow:**
1. User question passed through `RunnablePassthrough()`
2. Retriever fetches relevant chunks based on question
3. Context and question injected into prompt template
4. LLM generates structured review
5. `StrOutputParser()` extracts string output

### Performance Metrics

The system tracks:
- **Processing time**: End-to-end execution duration
- **Document statistics**: Page count (No-RAG) or chunk count (RAG)
- **Token count**: Estimated tokens for No-RAG approach
- **Truncation warnings**: Flagged when document exceeds limits

## Gradio Interface

**Input Components:**
- `gr.File`: PDF upload with `.pdf` file type filter, returns filepath
- `gr.Dropdown`: Approach selection (RAG/No-RAG)

**Output Components:**
- `gr.Textbox`: Review output (22 lines)
- `gr.Textbox`: Metrics output (10 lines)

**Interaction Flow:**
```
File Upload → Approach Selection → Button Click → 
process_pdf() → (review_text, metrics_text) → UI Update
```

## Deployment Context

**Status**: Pilot phase with authenticated access  
**Users**: Faculty and research development staff  
**Use Case**: Draft-review stage supplementary tool  
**Oversight**: Human-in-the-loop validation required

**Risk Mitigation:**
- No external API integration (data remains local)
- Strict access controls and audit logging
- Bias audits across disciplines
- Continuous monitoring and user feedback collection

## Dependencies

```
gradio
langchain-community
langchain-ollama
langchain-core
transformers
faiss-cpu (or faiss-gpu)
pypdf
torch
```

**Ollama Models Required:**
- `gemma3` (LLM)
- `nomic-embed-text` (embeddings)

## Installation & Execution

```bash
# Install dependencies
pip install gradio langchain-community langchain-ollama transformers faiss-cpu pypdf

# Install Ollama and pull models
ollama pull gemma3
ollama pull nomic-embed-text

# Run system
python D002_System.py
```

The Gradio interface launches with `share=True`, generating a public URL for remote access.
